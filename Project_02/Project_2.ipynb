{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55974225",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"ProjectII\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a35005b8",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "# TO MODIFY FOR YOUR SCHEMA\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"rate_code\", StringType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", FloatType(), True),\n",
    "    StructField(\"pickup_longitude\", FloatType(), True),\n",
    "    StructField(\"pickup_latitude\", FloatType(), True),\n",
    "    StructField(\"dropoff_longitude\", FloatType(), True),\n",
    "    StructField(\"dropoff_latitude\", FloatType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69712d38",
   "metadata": {},
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "# Load the DataFrame\n",
    ")\n",
    "df = lines.select(\"parsed_value.*\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "74a46c68-44ab-4e3a-90fb-d334423e4acc",
   "metadata": {},
   "source": [
    "### The project starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb9ee95c-d394-4542-a852-4e4fec0635d7",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TaxiUtilizationDashboard\") \\\n",
    "    .config(\"spark.sql.streaming.metricsEnabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.streaming.source.maxOffsetsPerTrigger\", 200) \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "677c8253-3d1b-4647-b062-3ac66a544771",
   "metadata": {},
   "source": [
    "csv_file_path = \"./split/\"\n",
    "\n",
    "stream_df = (spark.readStream\n",
    "    .format(\"csv\")\n",
    "    .schema(schema)\n",
    "    .option(\"header\", \"true\")  # Assuming the CSV file has a header\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Limit on max files to be processed per trigger interval\n",
    "    .load(csv_file_path))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b953fb7d-1450-4993-8e3a-004d5ff1d526",
   "metadata": {},
   "source": [
    "import time\n",
    "# if you create the table metastore before any data exists then the stream will result in an error as the table is generated with empty schema\n",
    "def create_table_if_exists(output_path,table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60): # you can replace this with while, currently timeouts after about 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\"))>0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{table_name}'\") # table metastore is created once there is some data (.parquet) in the directory\n",
    "                break\n",
    "        except Exception as e:\n",
    "            #print(e) # if you want to see the exceptions, uncomment this\n",
    "            pass"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03568f8-487c-4fef-a22e-cc4e9cf0af4a",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, window, when, expr, unix_timestamp\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c7e50c-074b-4963-a913-9490994b83dd",
   "metadata": {},
   "source": [
    "# Function to show the first 10 values of each micro-batch\n",
    "def show_first_10_values(batch_df, batch_id):\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    batch_df.show(10, truncate=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {},
   "source": [
    "## [Query 1] Utilization over a window of 5, 10, and 15 minutes per taxi/driver. This can be computed by computing the idle time per taxi. How does it change? Is there an optimal window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b2d7ed",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Calculate idle time in seconds\n",
    "stream_df = stream_df.withColumn(\"idle_time\", \n",
    "    when(col(\"pickup_datetime\").isNull() | col(\"dropoff_datetime\").isNull(), unix_timestamp(col(\"timestamp\")))\n",
    "    .otherwise(unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"dropoff_datetime\"))))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0038140-c0b4-4f95-b6b1-4fede5bcaf02",
   "metadata": {},
   "source": [
    "checkpoint_path = \"./checkpoint\"\n",
    "output_path_utilization = \"./streaming/question1/\"\n",
    "table_name_utilization = \"taxi_utilization\"\n",
    "\n",
    "if not os.path.exists(output_path_utilization):\n",
    "        os.makedirs(output_path_utilization)\n",
    "\n",
    "# Add watermark to handle late data\n",
    "\n",
    "# The watermark duration defines how late data can arrive for it to be considered for processing.\n",
    "# Setting the watermark duration to 20 minutes means that data arriving more than 20 minutes late will be considered too late and will be discarded.\n",
    "# In this context, a watermark duration of 20 minutes might have been used as a placeholder.\n",
    "query1_df = stream_df.withWatermark(\"timestamp\", \"20 minutes\")\n",
    "\n",
    "# Define window aggregations for 5, 10, and 15 minutes with sliding windows\n",
    "windowed_counts_5min = query1_df.groupBy(\n",
    "    col(\"medallion\"),\n",
    "    window(col(\"timestamp\"), \"5 minutes\", \"5 seconds\")\n",
    ").agg({\"idle_time\": \"sum\"}).withColumnRenamed(\"sum(idle_time)\", \"idle_time_5min\")\n",
    "\n",
    "windowed_counts_10min = query1_df.groupBy(\n",
    "    col(\"medallion\"),\n",
    "    window(col(\"timestamp\"), \"10 minutes\", \"5 seconds\")\n",
    ").agg({\"idle_time\": \"sum\"}).withColumnRenamed(\"sum(idle_time)\", \"idle_time_10min\")\n",
    "\n",
    "windowed_counts_15min = query1_df.groupBy(\n",
    "    col(\"medallion\"),\n",
    "    window(col(\"timestamp\"), \"15 minutes\", \"5 seconds\")\n",
    ").agg({\"idle_time\": \"sum\"}).withColumnRenamed(\"sum(idle_time)\", \"idle_time_15min\")\n",
    "\n",
    "# Use Append mode for writing the stream in Parquet format\n",
    "query_5min = (windowed_counts_5min.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", output_path_utilization + \"/5min\")\n",
    "    .foreachBatch(show_first_10_values)\n",
    "    .option(\"checkpointLocation\", checkpoint_path + \"/utilization_5min\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start())\n",
    "\n",
    "query_10min = (windowed_counts_10min.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", output_path_utilization + \"/10min\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path + \"/utilization_10min\")\n",
    "    .foreachBatch(show_first_10_values)\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start())\n",
    "\n",
    "query_15min = (windowed_counts_15min.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", output_path_utilization + \"/15min\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path + \"/utilization_15min\")\n",
    "    .foreachBatch(show_first_10_values)\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start())\n",
    "\n",
    "# Create the table if data exists\n",
    "create_table_if_exists(output_path_utilization + \"/5min\", table_name_utilization)\n",
    "create_table_if_exists(output_path_utilization + \"/10min\", table_name_utilization)\n",
    "create_table_if_exists(output_path_utilization + \"/15min\", table_name_utilization)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {},
   "source": [
    "## [Query 2] The average time it takes for a taxi to find its next fare(trip) per destination borough. This can be computed by finding the time difference, e.g. in seconds, between the trip's drop off and the next trip's pick up within a given unit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af55882d-6108-4c23-8e4f-1ae6ffdc4b75",
   "metadata": {},
   "source": [
    "!pip install geopandas "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2446fd8-fe3c-44f4-acd2-1fbdf395e3c8",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7084b-08bd-43a1-9db5-b03e72875ad4",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7783e8-59cf-4810-9641-3031b3b8dc32",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.functions import col, when, unix_timestamp, lead, avg, pandas_udf\n",
    "# Load the geojson file to get borough boundaries\n",
    "boroughs_gdf = gpd.read_file('nyc-boroughs.geojson')\n",
    "\n",
    "# Function to get the borough from coordinates using geopandas\n",
    "def get_borough(longitude, latitude):\n",
    "    point = Point(longitude, latitude)\n",
    "    for _, row in boroughs_gdf.iterrows():\n",
    "        if row['geometry'].contains(point):\n",
    "            return row['borough']\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Create a Pandas UDF to use with Spark\n",
    "@pandas_udf(StringType())\n",
    "def get_borough_udf(longitude, latitude):\n",
    "    return pd.Series([get_borough(lon, lat) for lon, lat in zip(longitude, latitude)])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50da3ac-3cf2-4fea-910c-e24e7c60f623",
   "metadata": {},
   "source": [
    "boroughs_gdf.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bea404b-fc76-48f9-83d9-5946617863de",
   "metadata": {},
   "source": [
    "# remember you can register another stream\n",
    "checkpoint_path = \"./checkpoint\"\n",
    "output_path_avg_fare_time = \"./streaming/question2/\"\n",
    "table_name = \"avg_time_taxi_utilization_per_borough\"\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "\n",
    "if not os.path.exists(output_path_avg_fare_time):\n",
    "        os.makedirs(output_path_avg_fare_time)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0962116b-705c-473d-80ec-7ecb01723a34",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, when, unix_timestamp, lead, avg, pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "# Derive destination borough using the UDF\n",
    "query2_df = stream_df.withColumn(\"destination_borough\", get_borough_udf(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))\n",
    "\n",
    "# Add watermark to handle late data\n",
    "query2_df = query2_df.withWatermark(\"dropoff_datetime\", \"20 minutes\")\n",
    "\n",
    "joined_stream = query2_df.alias(\"a\").join(\n",
    "    csv_stream_with_watermark.alias(\"b\"),\n",
    "    (col(\"a.medallion\") == col(\"b.medallion\")) &\n",
    "    (col(\"a.dropoff_datetime\") < col(\"b.pickup_datetime\")) &\n",
    "    (unix_timestamp(col(\"b.pickup_datetime\")) - unix_timestamp(col(\"a.dropoff_datetime\")) < 3600),\n",
    "    \"left_outer\"\n",
    ").select(\n",
    "    col(\"a.medallion\"),\n",
    "    col(\"a.destination_borough\"),\n",
    "    col(\"a.dropoff_datetime\"),\n",
    "    col(\"b.pickup_datetime\").alias(\"next_pickup_datetime\")\n",
    ").withColumn(\n",
    "    \"time_to_next_fare\", \n",
    "    unix_timestamp(col(\"next_pickup_datetime\")) - unix_timestamp(col(\"dropoff_datetime\"))\n",
    ").filter(\n",
    "    col(\"time_to_next_fare\").isNotNull() & (col(\"time_to_next_fare\") > 0)\n",
    ")\n",
    "\n",
    "# Compute the average time to next fare per destination borough\n",
    "avg_time_to_next_fare = joined_stream.groupBy(\"destination_borough\", window(col(\"dropoff_datetime\"), \"1 hour\"))\n",
    "        .agg(avg(\"time_to_next_fare\")\n",
    "        .alias(\"avg_time_to_next_fare\"))\n",
    "\n",
    "# Function to show the first 10 values of each micro-batch\n",
    "def show_first_10_values(batch_df, batch_id):\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    batch_df.show(10, truncate=False)\n",
    "\n",
    "# Write stream with foreachBatch to show the first 10 values\n",
    "query_avg_fare_time = (avg_time_to_next_fare.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .foreachBatch(show_first_10_values)\n",
    "    .option(\"checkpointLocation\", checkpoint_path + \"/avg_fare_time\")\n",
    "    .trigger(processingTime=\"5 minutes\")\n",
    "    .start())\n",
    "\n",
    "# Create the table if data exists\n",
    "create_table_if_exists(output_path_avg_fare_time, table_name_avg_fare_time)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {},
   "source": [
    "## [Query 3] The number of trips that started and ended within the same borough in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba57b7a2-e16e-44a1-890f-cd284c1cc1da",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, when, unix_timestamp, pandas_udf, window, count\n",
    "\n",
    "checkpoint_path = \"./checkpoint/\"\n",
    "output_path_same_borough = \"./streaming/question3\"\n",
    "table_name_same_borough = \"same_borough\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {},
   "source": [
    "#Derive pickup and dropoff borough using the UDF\n",
    "query3_df = stream_df.withColumn(\"pickup_borough\", get_borough_udf(col(\"pickup_longitude\"), col(\"pickup_latitude\")))\n",
    "query3_df = query3_df.withColumn(\"dropoff_borough\", get_borough_udf(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))\n",
    "\n",
    "# Add watermark to handle late data\n",
    "query3_df = query3_df.withWatermark(\"pickup_datetime\", \"1 hour\")\n",
    "\n",
    "# Filter trips that started and ended in the same borough\n",
    "same_borough_trips = query3_df.filter(col(\"pickup_borough\") == col(\"dropoff_borough\"))\n",
    "\n",
    "# Windowed aggregation to count trips in the last hour\n",
    "same_borough_count = same_borough_trips.groupBy(\n",
    "    window(col(\"pickup_datetime\"), \"1 hour\"),\n",
    "    col(\"pickup_borough\")\n",
    ").agg(count(\"*\").alias(\"trip_count\"))\n",
    "\n",
    "# Use Append mode for writing the stream in Parquet format\n",
    "query_same_borough = (same_borough_count.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", output_path_same_borough)\n",
    "    .option(\"checkpointLocation\", checkpoint_path + \"/same_borough\")\n",
    "    .foreachBatch(show_first_10_values)\n",
    "    .trigger(processingTime=\"1 minutes\")\n",
    "    .start())\n",
    "\n",
    "# Create the table if data exists\n",
    "create_table_if_exists(output_path_same_borough, table_name_same_borough)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c556a25-e46b-4b2f-b862-0f1561593b69",
   "metadata": {},
   "source": [
    "# Path to the Parquet file\n",
    "parquet_file_path = \"./streaming/question1/5min/part-00000-527d1fbd-ae29-43a8-a255-caadc98d3e98-c000.snappy.parquet\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "41d0d685-c3ed-4b7d-8ebc-c3174ba55645",
   "metadata": {},
   "source": [
    "## [Query 4] The number of trips that started in one borough and ended in another one in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d58fbf3-dc90-4987-8d5a-16a80c74f26f",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, when, unix_timestamp, pandas_udf, window, count\n",
    "\n",
    "checkpoint_path = \"./checkpoint/\"\n",
    "output_path_different_borough = \"./streaming/question4\"\n",
    "table_name_different_borough = \"different_borough\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f3578b-1960-4969-801b-adc2f45493a9",
   "metadata": {},
   "source": [
    "# Derive pickup and dropoff borough using the UDF\n",
    "query4_df = stream_df.withColumn(\"pickup_borough\", get_borough_udf(col(\"pickup_longitude\"), col(\"pickup_latitude\")))\n",
    "query4_df = query4_df.withColumn(\"dropoff_borough\", get_borough_udf(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))\n",
    "\n",
    "# Add watermark to handle late data\n",
    "query4_df = query4_df.withWatermark(\"pickup_datetime\", \"1 hour\")\n",
    "\n",
    "# Filter trips that started in one borough and ended in another\n",
    "different_borough_trips = query4_df.filter(col(\"pickup_borough\") != col(\"dropoff_borough\"))\n",
    "\n",
    "# Windowed aggregation to count trips in the last hour\n",
    "different_borough_count = different_borough_trips.groupBy(\n",
    "    window(col(\"pickup_datetime\"), \"1 hour\"),\n",
    "    col(\"pickup_borough\"),\n",
    "    col(\"dropoff_borough\")\n",
    ").agg(count(\"*\").alias(\"trip_count\"))\n",
    "\n",
    "# Use Append mode for writing the stream in Parquet format\n",
    "query_different_borough = (different_borough_count.writeStream\n",
    "    .outputMode(\"append\")  # Use complete mode to display all results\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", output_path_different_borough)\n",
    "    .option(\"checkpointLocation\", checkpoint_path + \"/different_borough\")\n",
    "    # .foreachBatch(show_first_10_values)\n",
    "    .trigger(processingTime=\"1 minutes\")\n",
    "    .start())\n",
    "\n",
    "# Create the table if data exists\n",
    "create_table_if_exists(output_path_different_borough, table_name_different_borough)\n",
    "\n",
    "# Wait for the termination signal for the query\n",
    "# query_different_borough.awaitTermination()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94aad69-f888-414f-b1e8-0bea827a22e0",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
